{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a RAG with Groq API's LLama 70b as base model and HuggingFace's sentence-transformer as embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain sentence-transformers langchain-community chromadb langchain-groq langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"HUGGINGFACE_AI_KEY\"] = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Create and populate the Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.1 : Load the text file using LangChain's TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data infestion for text files\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "text_documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.2 : (Optional) : If its data from a website that we need, then we have to scrape the web data. We can do that using Beautiful Soup module from Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scraping the data, we have to load it. This is done using the WebBaseLoader from LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data ingestion using web based loader\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_path =(\"https://www.bbc.com/sport/football/articles/cyrrjv3e4zro\"), # URL of the webpage\n",
    "    bs_kwargs=dict(parse_only = bs4.SoupStrainer(\n",
    "        class_=(\"zephr_join_beta\",\"ssrcss-181c4hk-SectionWrapper eustbbg0\",\n",
    "        \"ssrcss-4rxmy3-PageStack e1mcntqj2\",\"ssrcss-l6cntj-ContentStack e1mcntqj0\",\n",
    "        \"ssrcss-irv5dn-Zone e1mcntqj4\",\"ssrcss-irv5dn-Zone e1mcntqj4\",\n",
    "        \"ssrcss-1ocoo3l-Wrap e42f8511\",\"ssrcss-1y7k614-FooterStack e1mcntqj1\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "web_load = loader.load()\n",
    "print(web_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.3 : (Optional) : Lets imagine that the data we want if from a PDF. We can load the data in the file using PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data injestion using PDFs\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"jose's_plan.pdf\") # Path to the PDF file\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Chunk the data into smaller segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RecursiveCharacterTextSplitter splits the data into chunks of the specified chunk size and overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dividing the text into chunks\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 300, chunk_overlap = 100)\n",
    "final_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Ready the embedder from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector embeddings and Vector Store\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "ollama_emb = HuggingFaceEmbeddings() # huggingface embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = ollama_emb.embed_documents(\n",
    "    [\n",
    "        \"Alpha is the first letter of Greek alphabet\",\n",
    "        \"Beta is the second letter of Greek alphabet\",\n",
    "    ]\n",
    ")\n",
    "r1  # To check if it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Create the embeddings of the loaded data and insert it into the Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from chromadb.errors import InvalidDimensionException       # Store data into chroma db\n",
    "\n",
    "try:\n",
    "    db = Chroma.from_documents(final_docs,ollama_emb)\n",
    "except InvalidDimensionException:\n",
    "    Chroma().delete_collection()\n",
    "    db = Chroma.from_documents(final_docs,ollama_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if the database is ready by querying it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the playstyle of Tottenham Hotspur ?\"\n",
    "result = db.similarity_search(query)\n",
    "result      ## Similarity search for vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Prepare the LLM's API Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and ready the llm, here, we use groq api\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(temperature=0, model=\"llama3-70b-8192\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6 : (<b>MOST IMPORTANT PART<b>) : Pre-prompt is declared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Answer the following question based only the context provided.\n",
    "    Think step by step and provide a detailed answer.\n",
    "    Just give the final precise answer, no need to explain.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    Question : {input}\n",
    "    \"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7 : Create the chain between the Prompt, LLM and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8 : Query the model :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "input_question = input(\"Enter your question : \")\n",
    "\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever,document_chain)\n",
    "retrieval_chain.invoke({\"input\":input_question,\n",
    "                        \"context\":retriever})['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
